{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduzione\n",
    "\n",
    "Questo notebook ha l'obiettivo di fornire una panoramica visiva dell'algoritmo di discesa del gradiente, uno dei principali algoritmi di ottimizzazione utilizzati nell'apprendimento automatico per minimizzare la funzione di errore.\n",
    "\n",
    "Per fare ciò, utilizzeremo la funzione di errore MSE:\n",
    "\n",
    "$MSE = \\frac{1}{N} \\sum_{i=1}^{N}( (Y_i - \\hat{Y_i})^2 )$\n",
    "\n",
    "Inizieremo creando una distribuzione lineare con del rumore, al fine di mantenere i dati \"realistici\". Successivamente, utilizzeremo l'algoritmo di discesa del gradiente per ottimizzare i parametri di una semplice regressione lineare fatta manualmente, senza l'utilizzo di librerie di Machine Learning. \n",
    "\n",
    "In particolare, ci concentreremo sull'ottimizzazione del parametro __m__ _(il coefficiente angolare)_ della retta di regressione e terremo fisso al _termine noto_ il parametro __c__ _(distanza dall'origine)_, in modo da focalizzare il lavoro di ottimizzazione su un solo parametro e lavorare in uno spazio $R^2$.\n",
    "\n",
    "In questo modo, potremo analizzare l'andamento dell'errore sulla funzione di costo $L(m, \\overline{c})$ e visualizzare come il valore del parametro __m__ si muove verso il minimo della funzione di errore in ogni epoca. Inoltre, attraverso la visualizzazione dell'andamento della funzione di costo al variare del parametro m, potremo avere una panoramica visiva dell'andamento del parametro in casi particolari, come l'esplosione o la scomparsa del gradiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siamo pronti per iniziare!\n",
    "\n",
    "Facciamo l'import delle varie librerie che andremo ad utilizzare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import random\n",
    "from PIL import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inizializzazione del Dataset\n",
    "\n",
    "Iniziamo generando una distribuzione lineare con coefficiente angolare (slope) e una distanza dall'origine (intercept) creati randomicamente.\n",
    "\n",
    "Per dare senso ai dati rendendoli 'realistici' aggiungiamo all'equazione della distribuzione lineare un _rumore_ creato attraverso una distribuzione normale data da _np.random.normal()_\n",
    "\n",
    "__noise_factor__ controlla il rumore della distribuzione e verrà utilizzato come parametro di deviazione standard della distribuzione di rumore. Maggiore sarà il valore, maggiore sarà il la distanza dalla distribuzione normale dei punti di rumore\n",
    "\n",
    "Infine plottiamo il dataset ottenuto e scriviamo i valori \"reali\" del coefficiente angolare (slope) e della distanza dall'origine (intercept), che saranno poi i parametri __m__ e __c__ da identificare nella nostra regressione lineare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "from_x = 0\n",
    "to_x = 15\n",
    "\n",
    "# Parametro per controllare il rumore della distribuzione\n",
    "noise_factor = 3\n",
    "\n",
    "slope = random.uniform(0, 10)\n",
    "intercept = random.uniform(0, 10)\n",
    "\n",
    "X = np.linspace(from_x, to_x, num_points)\n",
    "Y = slope*X + intercept + np.random.normal(from_x, noise_factor, num_points)\n",
    "\n",
    "print (\"> real_slope (m): {} \\n> real_intercept (c): {}\". format(slope, intercept))\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inizializzazione dei parametri (m e c) e definizione degli iperparametri (lr e epochs)\n",
    "\n",
    "\n",
    "##### PARAMETRO M\n",
    "\n",
    "Andiamo a settare randomicamente il paremtro __m__ in modo tale che successivamente utilizzeremo la tecnica della discesa del gradiente per ottimizzare questo valore. Volendo possiamo settarlo anche a __zero__ come valore di partenza, ma settarlo ad un valore randomico aiuta poi successivamente ad apprezzare meglio il diverso comportamento del gradient descent in funzione del valore iniziale di __m__\n",
    "\n",
    "\n",
    "##### PARAMETRO C\n",
    "\n",
    "Per semplicità, il parametro __c__ lo fissiamo direttaemente noi al valore _noto_ reale, perché in questo contesto vogliamo tenere il lavoro singolarmente sul parametro __m__, in modo da lavorare in $R^2$ quando successivamente analizzeremo l'andamento dell'errore su $L(m, \\overline{c})$\n",
    "\n",
    "##### IPERPARAMETRI\n",
    "\n",
    "Gli iperparametri __lr__ _(Learning Rate)_ e __epochs__ _(numero di epoche)_ possono essere modificati per testare le conseguenze di queste modifiche sul comportamento del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------\n",
    "# Parametri\n",
    "#-------------------\n",
    "\n",
    "m = random.uniform(0, 100)\n",
    "#m = slope\n",
    "\n",
    "c = intercept\n",
    "\n",
    "#-------------------\n",
    "# Iper parametri\n",
    "#-------------------\n",
    "\n",
    "# learning Rate\n",
    "lr = 0.001 \n",
    "\n",
    "# Numero di epoche\n",
    "epochs = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definizione della funzione di errore e delle relative derivate\n",
    "\n",
    "Definiamo la funzione di funzione di errore e le relative derivate rispetto a m e rispetto a c\n",
    "\n",
    "In questo contesto utilizziamo la MSE.\n",
    "\n",
    "$MSE = \\frac{1}{N} \\sum_{i=1}^{N}( (Y_i - \\hat{Y_i})^2 )$\n",
    "\n",
    "Nel contesto di questo notebook, la derivata rispetto a c non verrà mai utilizzata, poiché come spiegato sopra, il parametro c è già definito sul termine noto e resterà costante per tutto il tempo (quindi non verrà ottimizzata dal gradient descent). Per completezza però, definiamo comunque la derivata della funzione di errore rispetto a c\n",
    "\n",
    "__Derivata rispetto a m__\n",
    "\n",
    "$\\frac{\\partial \\mathrm{MSE}}{\\partial m} = - \\frac{2}{N} \\sum_{i=1}^{N}( X_i(Y_i - \\hat{Y_i})^2 ) $\n",
    "\n",
    "__Derivata rispetto a c__\n",
    "\n",
    "$\\frac{\\partial \\mathrm{MSE}}{\\partial c} = - \\frac{2}{N} \\sum_{i=1}^{N}( (Y_i - \\hat{Y_i})^2 ) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione della funzione di errore. In questo contesto utilizziamo la MSE\n",
    "def f_error(n, m, c, X, Y):\n",
    "    return (1/n) * np.sum( (Y - m*X - c)**2 )\n",
    "\n",
    "\n",
    "def get_m_derivative(n, X, Y, Y_pred):\n",
    "    return (-2 / n) * sum(X * (Y - Y_pred)) \n",
    "\n",
    "def get_c_derivative(n, Y, Y_pred):\n",
    "    return (-2 / n) * sum(Y - Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo di discesa del gradiente e aggiornaemnto parametri\n",
    "\n",
    "Andiamo ad effettuare l'algoritmo di discesa del gradiente per il numero di epoche stabilito nella definizione degli iperparametri _(nota che per semplicità in questo esempio non c'è early stopping)_\n",
    "\n",
    "Ad ogni epoca:\n",
    "\n",
    "- calcoliamo il valore della predizione sulla base del valore corrente dei parametri __m__ e __c__\n",
    "- utilizzamo la predizione creata per calcolare le derivate della funzione di errore\n",
    "- aggiorniamo i parametei di conseguenza\n",
    "\n",
    "__Nota__ che in questo notebook, poiché teniamo il valore di __c__ fisso al suo valore _reale_, non andiamo a calcolare la derivata della funzione di errore rispetto a __c__ e non andiamo ad aggiornare il valore del parametro __c__ (in quanto questo è già ottimizzato manualmente)\n",
    "\n",
    "\n",
    "Infine stampiamo alcune informazioni utili per valutare come si è comportato il processo di _fit_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# numero di esempi del dataset\n",
    "n = len(X)  \n",
    "\n",
    "# array che conterrà i vari valori assunti dal parametro m dopo ogni epoca\n",
    "M = [] \n",
    "\n",
    "# array che conterrà i vari valori assunti dall'errore dopo ogni epoca\n",
    "errors = []\n",
    "\n",
    "#-------------------------\n",
    "# Discesa del Gradiente\n",
    "#----------------------\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Append sugli array M e errors\n",
    "    errors.append(f_error(n, m, c, X, Y))\n",
    "    M.append(m)\n",
    "    \n",
    "    \n",
    "    # predizioni di y\n",
    "    Y_pred = m * X + c  \n",
    "        \n",
    "        \n",
    "    # derivata prima rispetto ad m e aggiornamento parametro m\n",
    "    D_m =  get_m_derivative(n, X, Y, Y_pred)\n",
    "    m -= lr * D_m  \n",
    "    \n",
    "    \n",
    "    #-------------------\n",
    "    # Questa parte qui è disabilitata perché fisso già io C perché voglio solo vedere\n",
    "    # la variazione di M con C conosciuto\n",
    "    #-------------------\n",
    "    # derivata prima rispetto ad c e aggiornamento parametro c\n",
    "    # D_c = get_m_derivative(n, Y, Y_pred)\n",
    "    # c -= lr * D_c \n",
    "    #-------------------\n",
    "    \n",
    "\n",
    "print (\"> real_slope (m): {} \\n> real_intercept (c): {}\". format(slope, intercept))\n",
    "print(\"---\")\n",
    "print (\"> predicted_slope (m): {} \\n> known_intercept (c): {}\". format(m, c))\n",
    "print(\"---\")\n",
    "print (\"> Min Error: {} \".format(min(errors) ) )\n",
    "\n",
    "\n",
    "# Predizioni\n",
    "Y_pred = m * X + c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto, per valutare meglio l'andamento del modello, procediamo con il plottare\n",
    "\n",
    "- la retta predetta in relazione ai dati _reali_\n",
    "\n",
    "- l'andamemento ad ogni epoca dei valori di errore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot of real data and predicted regression line\n",
    "axs[0].scatter(X, Y)\n",
    "axs[0].plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='red')\n",
    "axs[0].set_xlabel(\"X\")\n",
    "axs[0].set_ylabel(\"Y\")\n",
    "axs[0].set_title(f\"Regression Y: {m:.3e}*x + {c:.3e}\")\n",
    "\n",
    "# Plot of error vs epoches\n",
    "axs[1].plot(errors)\n",
    "axs[1].set_xlabel(\"Epoches\")\n",
    "axs[1].set_ylabel(\"Error\")\n",
    "axs[1].set_title(f'Errors | Max Err {max(errors):.3e} | Min Err {min(errors):.3e}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizziamo l'andamento della funzione di costo $L(m)$ al variare di $m$\n",
    "\n",
    "\n",
    "__Andiamo a creare un plot che ci mostra__\n",
    "\n",
    "\n",
    "- la funzione di costo $L(m)$ in un intervallo sull'asse orizzontale che va da __max(m)__ a __min(m)__\n",
    "\n",
    "\n",
    "- i valori di $L(m)$ assunti con il il variare del coefficiente angolare $m$ nelle varie epoche\n",
    "\n",
    "Questo ci permetterà di avere una overview complessiva di come l'errore in ogni epoca si è mosso all'interno della funzione di errore. I tre casi principali sono:\n",
    "\n",
    "- __Comprotamento nomrale__ i valori assunti da $L(m)$ al variare di $m$ tendono a scendere verso il punto di minimo di $L(m)$\n",
    "\n",
    "- __Esplosione del gradiente__ i valori assunti da $L(m)$ al variare di $m$ vanno verso gli estremi di $L(m)$\n",
    "\n",
    "- __Scomparsa del gradiente__ i valori assunti da $L(m)$ al variare di $m$ si muovono in un intervallo $m$ molto ristretto e gli estremi _min(m)_ e _max(m)_ sono molto vicini tra loro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis_min = min(M) - abs(min(M))*0.1\n",
    "x_axis_max = max(M) + abs(max(M))*0.1\n",
    "\n",
    "\n",
    "f_error_dict = {}\n",
    "\n",
    "# Define the range of values for m\n",
    "horiz = np.linspace(x_axis_min , x_axis_max, len(M)  )\n",
    "\n",
    "# Calculate the function values for each m value\n",
    "f_error_values = [ f_error(n, elem, c, X, Y) for elem in horiz]\n",
    "\n",
    "plt.xlim(x_axis_min, x_axis_max)\n",
    "\n",
    "\n",
    "# Plot the function values\n",
    "plt.plot(horiz, f_error_values)\n",
    "\n",
    "\n",
    "# plot the scatter plot\n",
    "plt.scatter(M, errors, c=cm.copper(M))\n",
    "\n",
    "    \n",
    "plt.xlabel('m')\n",
    "plt.ylabel('L(m)')\n",
    "plt.title(f'Gradient Descent | Max Err {max(errors):.3e} | Min Err {min(errors):.3e}')\n",
    "\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animiamo l'andamento della funzione di costo $L(m)$ al variare di $m$\n",
    "\n",
    "Andiamo a ricreare il plot di cui sopra che ci mostra in maniera animata tramite una gif, l'andamento dei valori di $L(m)$ assunti con il il variare del coefficiente angolare $m$ nelle varie epoche\n",
    "\n",
    "\n",
    "Questo ci permetterà di avere una overview complessiva di come l'errore in ogni epoca si è mosso all'interno della funzione di errore. I tre casi principali sono:\n",
    "\n",
    "- __Comprotamento nomrale__ i valori assunti da $L(m)$ al variare di $m$ tendono a scendere verso il punto di minimo di $L(m)$\n",
    "\n",
    "- __Esplosione del gradiente__ i valori assunti da $L(m)$ al variare di $m$ tendono a _saltare_ da una parte all'altra di $L(m)$\n",
    "\n",
    "- __Scomparsa del gradiente__ i valori assunti da $L(m)$ al variare di $m$ si muovono talmente lentamente su $L(m)$ da sembrare immobili\n",
    "\n",
    "\n",
    "__P.S.:__ dato che non avevo mai lavorato con le gif, parte di questo codice è stato fatto con l'ausilio di ChatGPT che ringuazio per il tempo che mi ha risparmiato su diverse cose di questo notebook :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('m')\n",
    "ax.set_ylabel('L(m)')\n",
    "\n",
    "\n",
    "# Initialize empty list for frames\n",
    "frames = []\n",
    "\n",
    "\n",
    "print('Starting creating the gif frames')\n",
    "print('---')\n",
    "\n",
    "# Create the scatter plot for each point\n",
    "for i in range(len(M)):\n",
    "\n",
    "    \n",
    "    x_axis_min = M[i] - abs(M[i])*0.5\n",
    "    x_axis_max = M[i] + abs(M[i])*0.5\n",
    "\n",
    "\n",
    "    # Define the range of values for m\n",
    "    horiz = np.linspace(x_axis_min , x_axis_max, len(M)  )\n",
    "    \n",
    "\n",
    "    # Calculate the function values for each m value\n",
    "    f_error_values = [ f_error(n, elem, c, X, Y) for elem in horiz]\n",
    "    ax.plot(horiz, f_error_values, zorder=1)\n",
    "\n",
    "    \n",
    "    # Create lists of x and y values for scatter plot\n",
    "    scatter_x = M[i]\n",
    "    scatter_y = scatter_y = errors[i]\n",
    "\n",
    "    \n",
    "    # Plot the scatter plot\n",
    "    ax.scatter(scatter_x, scatter_y, zorder=2, color='red')\n",
    "\n",
    "    # Save the current plot as a PIL image\n",
    "    fig.canvas.draw()\n",
    "    frame = Image.frombytes('RGB', fig.canvas.get_width_height(), fig.canvas.tostring_rgb())\n",
    "    frames.append(frame)\n",
    "\n",
    "    # Clear the plot for the next iteration\n",
    "    ax.clear()\n",
    "\n",
    "    # set the tick density on the X and Y axes\n",
    "    ax.locator_params(axis='x', nbins=10)\n",
    "    ax.locator_params(axis='y', nbins=10)\n",
    "\n",
    "    ax.set_xlabel('m')\n",
    "    ax.set_ylabel('L(m)')\n",
    "    ax.set_title(f'Epoch: {i} | m: {scatter_x:.2e} | L(m): {scatter_y:.2e}')\n",
    "    \n",
    "    print(f'Epoch: {i} | m: {scatter_x:.2e} | L(m): {scatter_y:.2e}')\n",
    "    \n",
    "    \n",
    "    # Close the plot to avoid displaying it\n",
    "    plt.close()\n",
    "\n",
    "print('---')\n",
    "print('saving the gif (it can take some time, especially with many epoches)')\n",
    "print('Please waiting...')\n",
    "print('The gif has been successfully created!')\n",
    "\n",
    "# Save the gif\n",
    "frames[0].save('gradient_descent.gif', format='GIF', append_images=frames[1:], save_all=True, duration=300, loop=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
